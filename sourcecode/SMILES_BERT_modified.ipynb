{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"id": "W45ZIned0r2L"}, "outputs": [], "source": ["import pandas as pd\n", "import torch\n", "import torch.nn.functional as F\n", "from torch import nn\n", "from torch.utils.data import DataLoader, TensorDataset\n", "from transformers import BertTokenizerFast, BertModel"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "1fhBGyTh_PbU"}, "outputs": [], "source": ["# 1. \ub370\uc774\ud130 \ub85c\ub4dc\n", "file_path = r'C:\\Users\\user\\Desktop\\WorkSpace\\DACON_drugdevproject\\data\\train.csv'\n", "train_data = pd.read_csv(file_path)\n", "\n", "file_path = r'C:\\Users\\user\\Desktop\\WorkSpace\\DACON_drugdevproject\\data\\test.csv'\n", "test_data = pd.read_csv(file_path)"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "x2StSDPTAU-t"}, "outputs": [], "source": ["# 2. \ud544\uc694\ud55c \uc5f4\ub9cc \ucd94\ucd9c\n", "dataset = train_data[['Smiles', 'IC50_nM']]"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "XE8_MGKJAkK8"}, "outputs": [], "source": ["# 3. \ud1a0\ud06c\ub098\uc774\uc800 \ub85c\ub4dc\n", "checkpoint = 'unikei/bert-base-smiles'\n", "tokenizer = BertTokenizerFast.from_pretrained(checkpoint)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "B643vQLNQ48Z"}, "outputs": [], "source": ["# 4. SMILES \ub370\uc774\ud130\ub97c \ud1a0\ud070\ud654\n", "smiles_list = dataset['Smiles'].tolist()\n", "tokenized_inputs = tokenizer(smiles_list, padding=True, truncation=True, return_tensors=\"pt\")"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "OWBqLaIAUquo"}, "outputs": [], "source": ["# 5. IC50 \uac12\uc744 \ud150\uc11c\ub85c \ubcc0\ud658\n", "ic50_values = dataset['IC50_nM'].tolist()\n", "labels = torch.tensor(ic50_values, dtype=torch.float)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "_bg_rNMvUwqR"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["input_ids shape: torch.Size([16, 96])\n", "attention_mask shape: torch.Size([16, 96])\n", "labels shape: torch.Size([16])\n"]}], "source": ["# 6. \ub370\uc774\ud130\uc14b \uc0dd\uc131\n", "train_dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], labels)\n", "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n", "for batch in train_dataloader:\n", "        input_ids, attention_mask, ic50_labels = batch\n", "        print(f\"input_ids shape: {input_ids.shape}\")  # (batch_size, seq_len)\n", "        print(f\"attention_mask shape: {attention_mask.shape}\")  # (batch_size, seq_len)\n", "        print(f\"labels shape: {ic50_labels.shape}\")  # (batch_size,)\n", "        break"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"id": "uFtFBrF0VXJF"}, "outputs": [], "source": ["class EncoderOnlyTransformer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=768, nhead=12, num_encoder_layers=12, dim_feedforward=3072, dropout=0.1):\n", "        super(EncoderOnlyTransformer, self).__init__()\n", "        # \ud1a0\ud070 ID\ub97c \uc784\ubca0\ub529 \ucc28\uc6d0\uc73c\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud55c \uc784\ubca0\ub529 \ub808\uc774\uc5b4\n", "        self.embedding = nn.Embedding(vocab_size, d_model)\n", "        \n", "        \n", "        # Transformer Encoder Layer\n", "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n", "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n", "\n", "        # Positional encoding\n", "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, d_model))  # Assuming max seq_len = 512\n", "        \n", "        # Affine (Linear) layer for regression (output 1 dimension)\n", "        self.regressor = nn.Linear(d_model, 1)\n", "        \n", "        # Activation\n", "        self.relu = nn.ReLU()\n", "\n", "    def forward(self, input_ids, attention_mask=None):\n", "        # \uc784\ubca0\ub529 \ub808\uc774\uc5b4 \uc801\uc6a9 (input_ids: [batch_size, seq_len] -> [batch_size, seq_len, d_model])\n", "        embedded_input = self.embedding(input_ids)\n", "        if torch.isnan(embedded_input).any():\n", "            print(\"Embedding layer\uc5d0\uc11c NaN \ubc1c\uc0dd\")\n", "        # Apply positional encoding\n", "        seq_len = embedded_input.size(1)\n", "        inputs_with_position = embedded_input + self.positional_encoding[:, :seq_len, :]\n", "\n", "        # attention_mask\ub97c bool \ud0c0\uc785\uc73c\ub85c \ubcc0\ud658 (0 -> False, 1 -> True)\n", "        if attention_mask is not None:\n", "            attention_mask = attention_mask.bool()\n", "\n", "        # Transformer encoder expects input as [batch_size, seq_len, d_model]\n", "        encoded_output = self.transformer_encoder(inputs_with_position)\n", "\n", "        # Apply pooling on the sequence dimension (e.g., using the output of the [CLS] token or average pooling)\n", "        pooled_output = torch.mean(encoded_output, dim=1)  # Simple average pooling\n", "\n", "        # Apply ReLU activation\n", "        pooled_output = self.relu(pooled_output)\n", "\n", "        # Apply Affine layer for regression\n", "        ic50_preds = self.regressor(pooled_output)\n", "        \n", "        return ic50_preds\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "vP1H34oS0r2P", "outputId": "0957eeb7-421a-4095-c832-feb13bf5932d"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["True\n", "NVIDIA GeForce RTX 3050\n"]}], "source": ["print(torch.cuda.is_available())  # True\uac00 \ub098\uc640\uc57c \uc815\uc0c1\n", "print(torch.cuda.get_device_name(0))  # \uccab \ubc88\uc9f8 GPU \uc774\ub984 \ucd9c\ub825"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"colab": {"background_save": true}, "id": "TLKH3jUGWeo2", "outputId": "4633cc76-433e-4b29-d5ea-1ffba3c69a25"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [1/20], Loss: 7132199.6368\n", "Epoch [2/20], Loss: 6974961.8545\n", "Epoch [3/20], Loss: 6973385.3932\n", "Epoch [4/20], Loss: 6972908.8961\n", "Epoch [5/20], Loss: 6972466.8060\n", "Epoch [6/20], Loss: 6973344.2765\n", "Epoch [7/20], Loss: 6975956.5033\n", "Epoch [8/20], Loss: 6974144.0517\n", "Epoch [9/20], Loss: 6970526.6675\n", "Epoch [10/20], Loss: 6970071.9188\n", "Epoch [11/20], Loss: 6967670.2723\n", "Epoch [12/20], Loss: 6971723.1785\n", "Epoch [13/20], Loss: 6972133.3463\n", "Epoch [14/20], Loss: 6972833.0236\n", "Epoch [15/20], Loss: 6973010.1268\n", "Epoch [16/20], Loss: 6975412.0237\n", "Epoch [17/20], Loss: 6974001.7897\n", "Epoch [18/20], Loss: 6974686.0300\n", "Epoch [19/20], Loss: 6970714.4038\n", "Epoch [20/20], Loss: 6972421.2377\n", "\ud559\uc2b5 \uc644\ub8cc!\n"]}, {"ename": "", "evalue": "", "output_type": "error", "traceback": ["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n", "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n", "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n", "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}], "source": ["import torch\n", "from torch.utils.data import DataLoader, TensorDataset\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "\n", "# \ub370\uc774\ud130 \ub85c\ub354\ub294 \uc774\ubbf8 \ub9cc\ub4e4\uc5b4\uc84c\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4\n", "# train_dataloader: \ud6c8\ub828 \ub370\uc774\ud130\ub85c\ubd80\ud130 \uac00\uc838\uc628 \ubc30\uce58\n", "# test_dataloader: \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub85c\ubd80\ud130 \uac00\uc838\uc628 \ubc30\uce58\n", "\n", "# \ubaa8\ub378 \ucd08\uae30\ud654\n", "model = EncoderOnlyTransformer(vocab_size = tokenizer.vocab_size, d_model=768, nhead=6, num_encoder_layers=6, dim_feedforward=1024, dropout=0.1)\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "model.to(device)\n", "\n", "# \uc190\uc2e4 \ud568\uc218 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800\n", "loss_fn = nn.MSELoss()\n", "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n", "\n", "# \ud559\uc2b5 \uc124\uc815\n", "num_epochs = 20\n", "train_loss_history = []\n", "\n", "# \ud559\uc2b5 \ub8e8\ud504\n", "for epoch in range(num_epochs):\n", "    model.train()  # \ubaa8\ub378\uc744 \ud559\uc2b5 \ubaa8\ub4dc\ub85c \uc124\uc815\n", "    epoch_loss = 0.0\n", "    \n", "    for batch in train_dataloader:\n", "        input_ids, attention_mask, labels = batch\n", "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n", "        \n", "        # \ubaa8\ub378 \uc608\uce21\n", "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n", "        outputs = outputs.squeeze()  # (batch_size, 1) -> (batch_size,)\ub85c \ubcc0\uacbd\n", "        \n", "        # \uc190\uc2e4 \uacc4\uc0b0\n", "        loss = loss_fn(outputs, labels)\n", "        \n", "        # \uc5ed\uc804\ud30c \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc5c5\ub370\uc774\ud2b8\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "        \n", "        epoch_loss += loss.item()\n", "        \n", "    \n", "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n", "    train_loss_history.append(avg_epoch_loss)\n", "    \n", "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")\n", "\n", "print(\"\ud559\uc2b5 \uc644\ub8cc!\")\n"]}, {"cell_type": "code", "metadata": {}, "source": "\n# Test data tokenization\ntest_smiles_list = test_data['Smiles'].tolist()\ntest_tokenized_inputs = tokenizer(test_smiles_list, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Load the trained model (assuming the model is already trained and available)\n# model = EncoderOnlyTransformer(...)  # The model should be defined and trained earlier\n\n# Put the model in evaluation mode\nmodel.eval()\n\n# Move the model and data to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntest_tokenized_inputs = {key: value.to(device) for key, value in test_tokenized_inputs.items()}\n\n# Make predictions\nwith torch.no_grad():\n    ic50_predictions = model(test_tokenized_inputs['input_ids'], test_tokenized_inputs['attention_mask'])\n\n# Convert predictions to CPU and detach\nic50_predictions = ic50_predictions.cpu().numpy()\n\n# Save the predictions to a CSV file\ntest_data['Predicted_IC50_nM'] = ic50_predictions\noutput_file = '/mnt/data/test_predictions.csv'\ntest_data.to_csv(output_file, index=False)\n\nprint(f\"Predictions saved to {output_file}\")\n", "outputs": [], "execution_count": null}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 0}